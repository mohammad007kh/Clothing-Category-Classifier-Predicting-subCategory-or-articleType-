{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothing Category Classifier\n",
    "(Predicting subCategory or articleType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from huggingface\n",
    "dataset = load_dataset(\"nreimers/fashion-dataset\")\n",
    "print(dataset)\n",
    "\n",
    "# Converting to pandas DataFrame\n",
    "df = dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant clothing items\n",
    "relevant_clothing = [\n",
    "    \"Sweaters\", \"Jackets\", \"Mufflers\", \"Scarves\", \"Gloves\", \"Rain Jacket\", \n",
    "    \"Rain Trousers\", \"Boots\", \"Hats\", \"Trousers\", \"Tshirts\", \"Jeans\", \"Shirts\", \n",
    "    \"Track Pants\", \"Shorts\", \"Socks\", \"Dresses\", \"Skirts\"\n",
    "]\n",
    "\n",
    "# Filter dataset to keep only relevant clothing items\n",
    "df = df[df[\"articleType\"].isin(relevant_clothing)]\n",
    "\n",
    "# Handle missing season values by filling based on articleType\n",
    "season_mapping = {\n",
    "    \"Sweaters\": \"Winter\", \"Jackets\": \"Winter\", \"Mufflers\": \"Winter\", \"Scarves\": \"Winter\",\n",
    "    \"Gloves\": \"Winter\", \"Rain Jacket\": \"Fall\", \"Rain Trousers\": \"Fall\", \"Boots\": \"Winter\",\n",
    "    \"Hats\": \"Summer\", \"Trousers\": \"Fall\", \"Tshirts\": \"Summer\", \"Jeans\": \"Fall\",\n",
    "    \"Shirts\": \"Summer\", \"Track Pants\": \"Winter\", \"Shorts\": \"Summer\", \"Socks\": \"Winter\",\n",
    "    \"Dresses\": \"Spring\", \"Skirts\": \"Spring\"\n",
    "}\n",
    "df[\"season\"] = df[\"season\"].fillna(df[\"articleType\"].map(season_mapping))\n",
    "\n",
    "# Handle missing 'usage' values (if useful)\n",
    "df[\"usage\"] = df[\"usage\"].fillna(\"Unknown\")\n",
    "\n",
    "## Fill missing baseColour with 'Unknown'\n",
    "df['baseColour'].fillna('Unknown', inplace=True)\n",
    "\n",
    "## Fill missing year with the most common year\n",
    "df['year'].fillna(df['year'].mode()[0], inplace=True)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"filtered_fashion_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values to make sure we've handled them all\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing to deepen the knowledge about the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Seasons\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='season')\n",
    "plt.title('Distribution of Seasons')\n",
    "plt.show()\n",
    "\n",
    "# Category Coverage\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(data=df, x='articleType')\n",
    "plt.title('Category Coverage')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Gender Representation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='gender')\n",
    "plt.title('Gender Representation')\n",
    "plt.show()\n",
    "\n",
    "# Color Distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(data=df, x='baseColour')\n",
    "plt.title('Color Distribution')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Usage Column Analysis\n",
    "print(\"Unique values in 'usage' column:\")\n",
    "print(df['usage'].value_counts())\n",
    "\n",
    "# Visualize Usage Column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='usage')\n",
    "plt.title('Usage Column Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we've done it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(df.head())  # View first few rows\n",
    "print(df.info())  # Check column data types\n",
    "print(df.isnull().sum())  # Check for missing values\n",
    "print(df['subCategory'].value_counts())  # Check class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id', 'productDisplayName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['gender', 'masterCategory',], drop_first=True)\n",
    "\n",
    "# Label encode articleType, subCategory, season, usage, and baseColour\n",
    "le_article = LabelEncoder()\n",
    "df['articleType'] = le_article.fit_transform(df['articleType'])\n",
    "\n",
    "le_subCategory = LabelEncoder()\n",
    "df['subCategory'] = le_subCategory.fit_transform(df['subCategory'])\n",
    "\n",
    "le_season = LabelEncoder()\n",
    "df['season'] = le_season.fit_transform(df['season'])\n",
    "\n",
    "le_usage = LabelEncoder()\n",
    "df['usage'] = le_usage.fit_transform(df['usage'])\n",
    "\n",
    "le_baseColour = LabelEncoder()\n",
    "df['baseColour'] = le_baseColour.fit_transform(df['baseColour'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values assigned\n",
    "print(df[['articleType', 'subCategory', 'season', 'usage', 'baseColour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn year to age for a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current year dynamically\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Calculate age by subtracting birth year from current year\n",
    "df['year'] = current_year - df['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imbalanced Data\n",
    "Some subcategories have very few instances. I decided to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a minimum threshold (e.g., categories with < 50 instances)\n",
    "min_count = 50\n",
    "\n",
    "# Filter categories that appear more than the threshold\n",
    "valid_subcategories = df['subCategory'].value_counts()[df['subCategory'].value_counts() >= min_count].index\n",
    "\n",
    "# Keep only valid categories\n",
    "df = df[df['subCategory'].isin(valid_subcategories)]\n",
    "\n",
    "# Print updated category distribution\n",
    "print(df['subCategory'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable (subCategory) and features (everything else)\n",
    "X = df.drop(columns=['subCategory'])\n",
    "y = df['subCategory']\n",
    "\n",
    "# Split into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the split\n",
    "print(y_train.value_counts())  # Ensure balanced distribution in training set\n",
    "print(y_test.value_counts())   # Ensure balanced distribution in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for scaling & training\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Normalize numerical features\n",
    "    ('classifier', LogisticRegression(max_iter=500))  # Train model\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The results obtained so far are promising, but we need to take this analysis to the next level. In the following parts of this notebook, we will explore deeper by performing:\n",
    "\n",
    "1. **Model Comparison**: Compare different machine learning models to find the best performing one.\n",
    "2. **Hyperparameter Tuning**: Optimize the hyperparameters of the selected model to improve performance.\n",
    "3. **Cross-Validation Analysis**: Perform cross-validation to ensure the model's robustness and generalizability.\n",
    "4. **Feature Importance Visualization**: Visualize the importance of different features in the model.\n",
    "5. **Confusion Matrix and Classification Metrics**: Generate a confusion matrix and other classification metrics to evaluate the model's performance.\n",
    "\n",
    "Let's dive into each of these steps in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison - Training and evaluating multiple models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "train_times = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = accuracy\n",
    "    train_times[name] = train_time\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training time: {train_time:.2f} seconds\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\\n\")\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "models_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': list(results.values()),\n",
    "    'Training Time (s)': list(train_times.values())\n",
    "})\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Model', y='Accuracy', data=models_df)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim([models_df.Accuracy.min() - 0.05, 1.0])\n",
    "\n",
    "# Plot training time comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Model', y='Training Time (s)', data=models_df)\n",
    "plt.title('Model Training Time Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"Best performing model: {best_model_name} with accuracy of {results[best_model_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for the Best Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(f\"Performing hyperparameter tuning for {best_model_name}...\")\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=best_model,\n",
    "    param_grid=param_grids[best_model_name],\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "tuned_model = grid_search.best_estimator_\n",
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"Tuned model accuracy on test set: {tuned_accuracy:.4f}\")\n",
    "print(f\"Improvement over base model: {tuned_accuracy - results[best_model_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Analysis\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Setup k-fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold CV on the tuned model\n",
    "print(\"Performing k-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(tuned_model, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Display cross-validation results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, k_folds+1), cv_scores, color='skyblue')\n",
    "plt.axhline(y=cv_scores.mean(), color='red', linestyle='-', label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('5-Fold Cross-Validation Results')\n",
    "plt.xticks(range(1, k_folds+1))\n",
    "plt.ylim([cv_scores.min() - 0.05, 1.0])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# For tree-based models (Random Forest)\n",
    "if 'Random Forest' in best_model_name:\n",
    "    importances = tuned_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Get feature names (assuming X_train is a DataFrame with column names)\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        features = X_train.columns\n",
    "    else:\n",
    "        features = [f'Feature {i}' for i in range(X_train.shape[1])]\n",
    "    \n",
    "    plt.title('Feature Importance (Random Forest)')\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices], align='center')\n",
    "    plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# For linear models (Logistic Regression)\n",
    "elif 'Logistic Regression' in best_model_name:\n",
    "    coef = tuned_model.coef_[0]\n",
    "    \n",
    "    # Get feature names\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        features = X_train.columns\n",
    "    else:\n",
    "        features = [f'Feature {i}' for i in range(X_train.shape[1])]\n",
    "    \n",
    "    # Sort coefficients\n",
    "    coef_sorted_idx = np.argsort(np.abs(coef))\n",
    "    top_features = 20  # Show top 20 features\n",
    "    \n",
    "    plt.title('Feature Importance (Logistic Regression)')\n",
    "    plt.barh(range(top_features), coef[coef_sorted_idx[-top_features:]])\n",
    "    plt.yticks(range(top_features), [features[i] for i in coef_sorted_idx[-top_features:]])\n",
    "    plt.xlabel('Coefficient magnitude')\n",
    "\n",
    "# For other models, use permutation importance\n",
    "else:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    perm_importance = permutation_importance(tuned_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
    "    \n",
    "    # Get feature names\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        features = X_train.columns\n",
    "    else:\n",
    "        features = [f'Feature {i}' for i in range(X_train.shape[1])]\n",
    "    \n",
    "    plt.title('Feature Importance (Permutation Importance)')\n",
    "    plt.bar(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx])\n",
    "    plt.xticks(range(len(sorted_idx)), [features[i] for i in sorted_idx], rotation=90)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Evaluation Metrics and Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For multi-class problems, evaluate precision, recall and F1 for each class\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# If it's a multi-class problem with many classes, show metrics for top misclassified classes\n",
    "if len(np.unique(y)) > 10:\n",
    "    print(\"\\nTop Misclassified Classes:\")\n",
    "    error_mask = y_test != y_pred_tuned\n",
    "    misclassified = pd.DataFrame({\n",
    "        'True': y_test[error_mask],\n",
    "        'Predicted': y_pred_tuned[error_mask]\n",
    "    })\n",
    "    misclass_counts = misclassified.groupby(['True', 'Predicted']).size().reset_index(name='Count')\n",
    "    misclass_counts = misclass_counts.sort_values('Count', ascending=False).head(10)\n",
    "    print(misclass_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
